{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chenjiqing/anaconda3/envs/Class/lib/python3.7/site-packages/tqdm/std.py:697: FutureWarning: The Panel class is removed from pandas. Accessing it from the top-level namespace will also be removed in the next version\n",
      "  from pandas import Panel\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk \n",
    "import nltk.corpus\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import string\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from sklearn import preprocessing\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import make_moons, make_circles, make_classification\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     /Users/chenjiqing/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/chenjiqing/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('words')\n",
    "nltk.download('stopwords')\n",
    "words = nltk.corpus.words.words()\n",
    "sw = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load review data, clean the reviews, and save as .csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_json('/Users/chenjiqing/Public/2020_Fall_term/data/Toys_and_Games_Reviews_training.json', lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1460818, 12)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>overall</th>\n",
       "      <th>verified</th>\n",
       "      <th>reviewTime</th>\n",
       "      <th>reviewerID</th>\n",
       "      <th>asin</th>\n",
       "      <th>reviewerName</th>\n",
       "      <th>reviewText</th>\n",
       "      <th>summary</th>\n",
       "      <th>unixReviewTime</th>\n",
       "      <th>vote</th>\n",
       "      <th>image</th>\n",
       "      <th>style</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>True</td>\n",
       "      <td>08 12, 2012</td>\n",
       "      <td>BDF1A2B47461BE6FD7B8AD896005E061</td>\n",
       "      <td>5FC455C18F7DD81A071E51B6B60A72BD</td>\n",
       "      <td>73D759DCDF02B4DDBD3FB7D29E88C6C3</td>\n",
       "      <td>I always look for this brand of cards because ...</td>\n",
       "      <td>durable cards</td>\n",
       "      <td>1344729600</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>False</td>\n",
       "      <td>01 24, 2011</td>\n",
       "      <td>91BDB36705C359620E8E0959DEFCFC79</td>\n",
       "      <td>5FC455C18F7DD81A071E51B6B60A72BD</td>\n",
       "      <td>80C0DC9C8FAB1C9BE0E1D5E83965360C</td>\n",
       "      <td>Other than KEM's or DaVinchi brands, these car...</td>\n",
       "      <td>some of the best Cards</td>\n",
       "      <td>1295827200</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>True</td>\n",
       "      <td>11 18, 2016</td>\n",
       "      <td>B14C8E6A493C844D2AEDCC61207BB6B1</td>\n",
       "      <td>5FC455C18F7DD81A071E51B6B60A72BD</td>\n",
       "      <td>C133754F62F0EB10D1FFF543786622DC</td>\n",
       "      <td>The better ive ever seen!!!! Excelent!</td>\n",
       "      <td>Five Stars</td>\n",
       "      <td>1479427200</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>True</td>\n",
       "      <td>03 7, 2016</td>\n",
       "      <td>D53FA93ED4A6B67662D920473DE9C7F7</td>\n",
       "      <td>5FC455C18F7DD81A071E51B6B60A72BD</td>\n",
       "      <td>59820C0C923F245A167B53E24FFBF5BB</td>\n",
       "      <td>get it</td>\n",
       "      <td>Five Stars</td>\n",
       "      <td>1457308800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>False</td>\n",
       "      <td>05 22, 2015</td>\n",
       "      <td>B59D69D00FEA8B83388D292010CAB2E9</td>\n",
       "      <td>5FC455C18F7DD81A071E51B6B60A72BD</td>\n",
       "      <td>2843F6C4145178E4F87A6CD895F4887F</td>\n",
       "      <td>Really nice cards. Will make great use of them...</td>\n",
       "      <td>playing cards</td>\n",
       "      <td>1432252800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   overall  verified   reviewTime                        reviewerID  \\\n",
       "0        5      True  08 12, 2012  BDF1A2B47461BE6FD7B8AD896005E061   \n",
       "1        5     False  01 24, 2011  91BDB36705C359620E8E0959DEFCFC79   \n",
       "2        5      True  11 18, 2016  B14C8E6A493C844D2AEDCC61207BB6B1   \n",
       "3        5      True   03 7, 2016  D53FA93ED4A6B67662D920473DE9C7F7   \n",
       "4        5     False  05 22, 2015  B59D69D00FEA8B83388D292010CAB2E9   \n",
       "\n",
       "                               asin                      reviewerName  \\\n",
       "0  5FC455C18F7DD81A071E51B6B60A72BD  73D759DCDF02B4DDBD3FB7D29E88C6C3   \n",
       "1  5FC455C18F7DD81A071E51B6B60A72BD  80C0DC9C8FAB1C9BE0E1D5E83965360C   \n",
       "2  5FC455C18F7DD81A071E51B6B60A72BD  C133754F62F0EB10D1FFF543786622DC   \n",
       "3  5FC455C18F7DD81A071E51B6B60A72BD  59820C0C923F245A167B53E24FFBF5BB   \n",
       "4  5FC455C18F7DD81A071E51B6B60A72BD  2843F6C4145178E4F87A6CD895F4887F   \n",
       "\n",
       "                                          reviewText                 summary  \\\n",
       "0  I always look for this brand of cards because ...           durable cards   \n",
       "1  Other than KEM's or DaVinchi brands, these car...  some of the best Cards   \n",
       "2             The better ive ever seen!!!! Excelent!              Five Stars   \n",
       "3                                             get it              Five Stars   \n",
       "4  Really nice cards. Will make great use of them...           playing cards   \n",
       "\n",
       "   unixReviewTime vote image style  \n",
       "0      1344729600  NaN   NaN   NaN  \n",
       "1      1295827200    4   NaN   NaN  \n",
       "2      1479427200  NaN   NaN   NaN  \n",
       "3      1457308800  NaN   NaN   NaN  \n",
       "4      1432252800  NaN   NaN   NaN  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Count review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>asin</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>000254CDA01E5AE839E7EE3F6FC1D2F1</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0002E81AED1B9840434EE28E39876BE4</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0003806D9BE7957D8AB9AF9EC04F52D0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00049B538C925B3EF702FFF6FBD48DFE</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0004D0377FDC88F2DD349170E36324E7</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               asin  count\n",
       "0  000254CDA01E5AE839E7EE3F6FC1D2F1     24\n",
       "1  0002E81AED1B9840434EE28E39876BE4     13\n",
       "2  0003806D9BE7957D8AB9AF9EC04F52D0      6\n",
       "3  00049B538C925B3EF702FFF6FBD48DFE      8\n",
       "4  0004D0377FDC88F2DD349170E36324E7      5"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# count review number\n",
    "df_count = df.groupby([\"asin\"])['reviewText'].count().reset_index(name=\"count\")\n",
    "df_count.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "hist = df_count.hist(bins = 100)\n",
    "plt.yscale('log')\n",
    "plt.title('')\n",
    "#plt.xlabel('review count')\n",
    "#plt.ylabel('log(# of products)')\n",
    "plt.savefig('review count.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples: 1460818 Number of Features: 12\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of samples: {} Number of Features: {}\".format(df.shape[0], df.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_df = df[df['reviewText'].notna()]\n",
    "df2 = clean_df[['asin', 'overall', 'reviewText', 'summary']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>asin</th>\n",
       "      <th>reviewText</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>000254CDA01E5AE839E7EE3F6FC1D2F1</td>\n",
       "      <td>The child who recieved them opened the box and...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0002E81AED1B9840434EE28E39876BE4</td>\n",
       "      <td>Great gift for my special needs cousin Blocks ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0003806D9BE7957D8AB9AF9EC04F52D0</td>\n",
       "      <td>I needed something to occupy kids indoors that...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00049B538C925B3EF702FFF6FBD48DFE</td>\n",
       "      <td>This is a very nicely detailed and painted die...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0004D0377FDC88F2DD349170E36324E7</td>\n",
       "      <td>Cute This bear is so cool and cute. The fabric...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               asin  \\\n",
       "0  000254CDA01E5AE839E7EE3F6FC1D2F1   \n",
       "1  0002E81AED1B9840434EE28E39876BE4   \n",
       "2  0003806D9BE7957D8AB9AF9EC04F52D0   \n",
       "3  00049B538C925B3EF702FFF6FBD48DFE   \n",
       "4  0004D0377FDC88F2DD349170E36324E7   \n",
       "\n",
       "                                          reviewText  \n",
       "0  The child who recieved them opened the box and...  \n",
       "1  Great gift for my special needs cousin Blocks ...  \n",
       "2  I needed something to occupy kids indoors that...  \n",
       "3  This is a very nicely detailed and painted die...  \n",
       "4  Cute This bear is so cool and cute. The fabric...  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avg = df2[['asin', 'overall']].groupby('asin').mean()\n",
    "# assian traget=1 if averaged overall>4.5 and target=0 otherwise\n",
    "avg2 = avg.assign(\n",
    "    Target = lambda dataframe: dataframe['overall'].map(lambda overall: \"1\" if overall >4.5 else \"0\") \n",
    ")\n",
    "avg2 = avg2.rename_axis(\"asin\").reset_index()\n",
    "avg2.columns = ['asin','overall_avg', 'Target']\n",
    "\n",
    "# select coloumns\n",
    "df3 = df2[['asin', 'overall', 'reviewText', 'summary']]\n",
    "\n",
    "df4 = df3.groupby('asin')['reviewText'].apply(' '.join).reset_index()\n",
    "df4.head()\n",
    "#df4.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(63017, 1)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Senti = pd.read_csv('dft_TextBlobSentAdded.csv')\n",
    "avgSenti = Senti[['asin', 'sentiment_scores']].groupby('asin').mean()\n",
    "avgSenti.head()\n",
    "avgSenti.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add target and count to df4\n",
    "dft = df4.merge(avg2, on = 'asin', how ='left')\n",
    "dft = dft.merge(df_count, on = 'asin', how ='left')\n",
    "dft = dft.merge(avgSenti, on = 'asin', how ='left')\n",
    "dft.head()\n",
    "dft.shape\n",
    "#dft.to_csv('df_combined_text.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>asin</th>\n",
       "      <th>reviewText</th>\n",
       "      <th>overall_avg</th>\n",
       "      <th>Target</th>\n",
       "      <th>count</th>\n",
       "      <th>sentiment_scores</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>000254CDA01E5AE839E7EE3F6FC1D2F1</td>\n",
       "      <td>The child who recieved them opened the box and...</td>\n",
       "      <td>4.625000</td>\n",
       "      <td>1</td>\n",
       "      <td>24</td>\n",
       "      <td>0.487121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0002E81AED1B9840434EE28E39876BE4</td>\n",
       "      <td>Great gift for my special needs cousin Blocks ...</td>\n",
       "      <td>2.538462</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>-0.223308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0003806D9BE7957D8AB9AF9EC04F52D0</td>\n",
       "      <td>I needed something to occupy kids indoors that...</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>0.464500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>00049B538C925B3EF702FFF6FBD48DFE</td>\n",
       "      <td>This is a very nicely detailed and painted die...</td>\n",
       "      <td>4.625000</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>0.133913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0004D0377FDC88F2DD349170E36324E7</td>\n",
       "      <td>Cute This bear is so cool and cute. The fabric...</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0.557400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63012</th>\n",
       "      <td>63012</td>\n",
       "      <td>63012</td>\n",
       "      <td>FFFAB16CD5964F60F014EFAA1210AFEE</td>\n",
       "      <td>These stickers are really adorable. The variet...</td>\n",
       "      <td>4.928571</td>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "      <td>0.517586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63013</th>\n",
       "      <td>63013</td>\n",
       "      <td>63013</td>\n",
       "      <td>FFFAB94C16EAEC9E1DEBF24E231C53BB</td>\n",
       "      <td>great little toy. &lt;div id=\"video-block-R3SMI4X...</td>\n",
       "      <td>4.380952</td>\n",
       "      <td>0</td>\n",
       "      <td>21</td>\n",
       "      <td>0.530857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63014</th>\n",
       "      <td>63014</td>\n",
       "      <td>63014</td>\n",
       "      <td>FFFB0C64CBC232115CE8FFF2BE870122</td>\n",
       "      <td>I am a collector of everything Firefly / Seren...</td>\n",
       "      <td>4.850000</td>\n",
       "      <td>1</td>\n",
       "      <td>20</td>\n",
       "      <td>0.529530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63015</th>\n",
       "      <td>63015</td>\n",
       "      <td>63015</td>\n",
       "      <td>FFFB5BB91B42C9A34457AD075B499DF9</td>\n",
       "      <td>This made a cute Easter gift, but was soon for...</td>\n",
       "      <td>4.090909</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>0.557400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63016</th>\n",
       "      <td>63016</td>\n",
       "      <td>63016</td>\n",
       "      <td>FFFE6F164253092524A23147EBA2EEB3</td>\n",
       "      <td>Again great issue, this one has some assembly....</td>\n",
       "      <td>4.800000</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0.557400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>63017 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0  Unnamed: 0.1                              asin  \\\n",
       "0               0             0  000254CDA01E5AE839E7EE3F6FC1D2F1   \n",
       "1               1             1  0002E81AED1B9840434EE28E39876BE4   \n",
       "2               2             2  0003806D9BE7957D8AB9AF9EC04F52D0   \n",
       "3               3             3  00049B538C925B3EF702FFF6FBD48DFE   \n",
       "4               4             4  0004D0377FDC88F2DD349170E36324E7   \n",
       "...           ...           ...                               ...   \n",
       "63012       63012         63012  FFFAB16CD5964F60F014EFAA1210AFEE   \n",
       "63013       63013         63013  FFFAB94C16EAEC9E1DEBF24E231C53BB   \n",
       "63014       63014         63014  FFFB0C64CBC232115CE8FFF2BE870122   \n",
       "63015       63015         63015  FFFB5BB91B42C9A34457AD075B499DF9   \n",
       "63016       63016         63016  FFFE6F164253092524A23147EBA2EEB3   \n",
       "\n",
       "                                              reviewText  overall_avg  Target  \\\n",
       "0      The child who recieved them opened the box and...     4.625000       1   \n",
       "1      Great gift for my special needs cousin Blocks ...     2.538462       0   \n",
       "2      I needed something to occupy kids indoors that...     5.000000       1   \n",
       "3      This is a very nicely detailed and painted die...     4.625000       1   \n",
       "4      Cute This bear is so cool and cute. The fabric...     5.000000       1   \n",
       "...                                                  ...          ...     ...   \n",
       "63012  These stickers are really adorable. The variet...     4.928571       1   \n",
       "63013  great little toy. <div id=\"video-block-R3SMI4X...     4.380952       0   \n",
       "63014  I am a collector of everything Firefly / Seren...     4.850000       1   \n",
       "63015  This made a cute Easter gift, but was soon for...     4.090909       0   \n",
       "63016  Again great issue, this one has some assembly....     4.800000       1   \n",
       "\n",
       "       count  sentiment_scores  \n",
       "0         24          0.487121  \n",
       "1         13         -0.223308  \n",
       "2          6          0.464500  \n",
       "3          8          0.133913  \n",
       "4          5          0.557400  \n",
       "...      ...               ...  \n",
       "63012     14          0.517586  \n",
       "63013     21          0.530857  \n",
       "63014     20          0.529530  \n",
       "63015     11          0.557400  \n",
       "63016      5          0.557400  \n",
       "\n",
       "[63017 rows x 8 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dft = pd.read_csv('df_combined_text.csv')\n",
    "dft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1    52\n",
      "0    48\n",
      "Name: Target, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1410     63\n",
       "4004      6\n",
       "7125     12\n",
       "7287      5\n",
       "3296      5\n",
       "         ..\n",
       "2957     38\n",
       "53156     7\n",
       "51351     7\n",
       "12211     7\n",
       "20225    23\n",
       "Name: count, Length: 100, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dft_small = dft.sample(n =100)\n",
    "print(dft_small.Target.value_counts())\n",
    "dft_small['count']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a count vectorizer object\n",
    "token = RegexpTokenizer(r'[a-zA-Z]+')\n",
    "count_vectorizer = CountVectorizer(lowercase=True,stop_words='english',ngram_range = (1,1),tokenizer = token.tokenize)\n",
    "\n",
    "# fit the count vectorizer using the text data\n",
    "text_counts = count_vectorizer.fit_transform(dft['reviewText'])\n",
    "\n",
    "# collect the vocabulary items used in the vectorizer\n",
    "# dictionary = count_vectorizer.vocabulary_.items()\n",
    "\n",
    "# words appear in documents.\n",
    "tokens = count_vectorizer.get_feature_names()\n",
    "#print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split traning data and testing data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    text_counts, dft['Target'], test_size=0.2, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag of Words-multinomial NB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultinomialNB Accuracy: 0.7145350682323072\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.61      0.66      5774\n",
      "           1       0.71      0.80      0.75      6830\n",
      "\n",
      "   micro avg       0.71      0.71      0.71     12604\n",
      "   macro avg       0.72      0.71      0.71     12604\n",
      "weighted avg       0.72      0.71      0.71     12604\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# from sklearn.model_selection import cross_val_score ## for cross-validation\n",
    "from sklearn import metrics\n",
    "\n",
    "clf = MultinomialNB().fit(X_train, y_train)\n",
    "\n",
    "predicted= clf.predict(X_test)\n",
    "\n",
    "print(\"MultinomialNB Accuracy:\",metrics.accuracy_score(y_test, predicted))\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test,predicted))\n",
    "\n",
    "# scores = cross_val_score(clf, text_counts, dft['Target'], cv=5) ## for cross-validation\n",
    "# print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2)) ## for cross-validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag of Words- logisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LASSO-LR Accuracy: 0.7711044112980007\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.73      0.75      5774\n",
      "           1       0.78      0.81      0.79      6830\n",
      "\n",
      "   micro avg       0.77      0.77      0.77     12604\n",
      "   macro avg       0.77      0.77      0.77     12604\n",
      "weighted avg       0.77      0.77      0.77     12604\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Import model\n",
    "from sklearn.linear_model import Lasso, LogisticRegression\n",
    "from sklearn import metrics\n",
    "#Create a Classifier\n",
    "clf2 = LogisticRegression(C=1, penalty='l1', solver='liblinear') # Linear Kernel\n",
    "\n",
    "#Train the model using the training sets\n",
    "clf2.fit(X_train, y_train)\n",
    "\n",
    "#Predict the response for test dataset\n",
    "y_pred2 = clf2.predict(X_test)\n",
    "# print(y_pred)\n",
    "print(\"LASSO-LR Accuracy:\",metrics.accuracy_score(y_test, y_pred2))\n",
    "# print(clf.decision_function(X_test))\n",
    "# print(clf.predict_proba(X_test))\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test,y_pred2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag of Words- logisticRegression-CrossValidation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn import metrics\n",
    "\n",
    "#Create a svm Classifier\n",
    "clf3 = LogisticRegressionCV(penalty='l1', solver='liblinear',cv=10) # Linear Kernel\n",
    "\n",
    "#Train the model using the training sets\n",
    "clf3.fit(X_train, y_train)\n",
    "#Predict the response for test dataset\n",
    "y_pred3 = clf3.predict(X_test)\n",
    "# print(y_pred2)\n",
    "print(\"logisticRegression-cross-validation Accuracy:\",metrics.accuracy_score(y_test, y_pred3))\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test,y_pred3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf=TfidfVectorizer()\n",
    "text_tf= tf.fit_transform(dft['reviewText'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 59838)\t0.03202725550420936\n",
      "  (0, 34013)\t0.020935217148514658\n",
      "  (0, 125011)\t0.050770217024558355\n",
      "  (0, 84462)\t0.02079476564084484\n",
      "  (0, 71347)\t0.0972614489184268\n",
      "  (0, 12031)\t0.0337512187348777\n",
      "  (0, 94733)\t0.04365437244001424\n",
      "  (0, 57076)\t0.03191264206170739\n",
      "  (0, 127485)\t0.07537186735762977\n",
      "  (0, 97615)\t0.04979221748757807\n",
      "  (0, 27081)\t0.05606635236335404\n",
      "  (0, 34145)\t0.04794696264067572\n",
      "  (0, 79468)\t0.019839643657088634\n",
      "  (0, 65100)\t0.02593333074705919\n",
      "  (0, 98621)\t0.05934564609848282\n",
      "  (0, 74035)\t0.022214204178012842\n",
      "  (0, 104710)\t0.05133020829860741\n",
      "  (0, 98607)\t0.040402145551324445\n",
      "  (0, 18698)\t0.06083717496056019\n",
      "  (0, 134304)\t0.04301043126314798\n",
      "  (0, 139348)\t0.0655237114760431\n",
      "  (0, 69612)\t0.029335807392840075\n",
      "  (0, 70962)\t0.02781983130321231\n",
      "  (0, 69869)\t0.022391595773542758\n",
      "  (0, 97601)\t0.0666507672379206\n",
      "  :\t:\n",
      "  (62992, 152987)\t0.4830799999999999\n",
      "  (62993, 152987)\t0.4406708333333333\n",
      "  (62994, 152987)\t0.5574\n",
      "  (62995, 152987)\t0.2520454545454545\n",
      "  (62996, 152987)\t0.5574000000000001\n",
      "  (62997, 152987)\t0.4851444444444448\n",
      "  (62998, 152987)\t0.2307616822429909\n",
      "  (62999, 152987)\t0.5573999999999999\n",
      "  (63000, 152987)\t0.44592\n",
      "  (63001, 152987)\t0.3981428571428571\n",
      "  (63002, 152987)\t0.44592\n",
      "  (63003, 152987)\t0.3628913043478262\n",
      "  (63004, 152987)\t0.30271818181818183\n",
      "  (63005, 152987)\t0.5574000000000001\n",
      "  (63006, 152987)\t0.4162375\n",
      "  (63007, 152987)\t0.44592\n",
      "  (63008, 152987)\t0.4162375\n",
      "  (63009, 152987)\t0.4604608695652174\n",
      "  (63010, 152987)\t0.4335333333333332\n",
      "  (63011, 152987)\t0.5574\n",
      "  (63012, 152987)\t0.5175857142857144\n",
      "  (63013, 152987)\t0.5308571428571428\n",
      "  (63014, 152987)\t0.5295300000000001\n",
      "  (63015, 152987)\t0.5574000000000001\n",
      "  (63016, 152987)\t0.5574\n"
     ]
    }
   ],
   "source": [
    "# add count into text_tf (sparse matrix)\n",
    "from scipy import sparse\n",
    "text_tf_count = sparse.hstack((text_tf,np.array(dft['count'])[:,None]))\n",
    "text_tf_senti = sparse.hstack((text_tf,np.array(dft['sentiment_scores'])[:,None]))\n",
    "text_tf_count_senti = sparse.hstack((text_tf_count,np.array(dft['sentiment_scores'])[:,None]))\n",
    "print(text_tf_count_senti)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train, X_test, y_train, y_test = train_test_split(text_tf, dft_small['Target'], test_size=0.2, random_state=2)\n",
    "X_train = text_tf_senti\n",
    "y_train = dft['Target']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7.0\n",
      "The scikit-learn version is 0.23.2.\n"
     ]
    }
   ],
   "source": [
    "# pip install imblearn\n",
    "# check version number\n",
    "import imblearn\n",
    "print(imblearn.__version__)\n",
    "import sklearn\n",
    "print('The scikit-learn version is {}.'.format(sklearn.__version__)) # The scikit-learn version should be 0.23.2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TFIDF-multinomial NB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultinomialNB Accuracy: 1.0\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00         1\n",
      "\n",
      "    accuracy                           1.00         1\n",
      "   macro avg       1.00      1.00      1.00         1\n",
      "weighted avg       1.00      1.00      1.00         1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf = MultinomialNB().fit(X_train, y_train)\n",
    "\n",
    "predicted= clf.predict(X_test)\n",
    "\n",
    "\n",
    "print(\"MultinomialNB Accuracy:\",metrics.accuracy_score(y_test, predicted))\n",
    "print(classification_report(y_test,predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TFIDF-BernoulliNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultinomialNB Accuracy: 0.6090923516344018\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.43      0.50      5774\n",
      "           1       0.61      0.76      0.68      6830\n",
      "\n",
      "    accuracy                           0.61     12604\n",
      "   macro avg       0.61      0.60      0.59     12604\n",
      "weighted avg       0.61      0.61      0.60     12604\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import BernoulliNB\n",
    "clf_BNB = BernoulliNB().fit(X_train, y_train)\n",
    "\n",
    "predicted_BNB= clf_BNB.predict(X_test)\n",
    "\n",
    "print(\"MultinomialNB Accuracy:\",metrics.accuracy_score(y_test, predicted_BNB))\n",
    "print(classification_report(y_test,predicted_BNB))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TFIDF-CategoricalNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import CategoricalNB\n",
    "clf_CateNB = CategoricalNB().fit(X_train.toarray(), y_train)\n",
    "\n",
    "predicted_CateNB= clf_CateNB.predict(X_test.toarray())\n",
    "\n",
    "print(\"MultinomialNB Accuracy:\",metrics.accuracy_score(y_test, predicted_CateNB))\n",
    "print(classification_report(y_test,predicted_CateNB))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TFIDF-GaussianProcessClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "kernel = 1.0 * RBF(1.0)\n",
    "\n",
    "gpc = GaussianProcessClassifier(kernel=kernel,random_state=0).fit(X_train.toarray(), y_train)\n",
    "y_pred_gpc = gpc.predict(X_test.toarray())\n",
    "# print(y_pred2)\n",
    "print(\"GaussianProcessClassifier-Accuracy:\",metrics.accuracy_score(y_test, y_pred_gpc))\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test,y_pred_gpc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TFIDF - logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.82 (+/- 0.01)\n",
      "Recall: 0.85 (+/- 0.01)\n",
      "F1-score: 0.83 (+/- 0.01)\n"
     ]
    }
   ],
   "source": [
    "#Import model\n",
    "from sklearn.linear_model import Lasso, LogisticRegression\n",
    "from sklearn import metrics\n",
    "import time\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "#Create a Classifier\n",
    "clf3 = LogisticRegression(C=1, penalty='l1', solver='liblinear') # Linear Kernel\n",
    "\n",
    "\n",
    "#Train the model using the training sets\n",
    "#clf3.fit(X_train, y_train)\n",
    "#Predict the response for test dataset\n",
    "#y_pred3 = clf3.predict(X_test)\n",
    "# print(y_pred3)\n",
    "#print(\"logisticRegression-Accuracy:\",metrics.accuracy_score(y_test, y_pred3))\n",
    "#from sklearn.metrics import classification_report\n",
    "#print(classification_report(y_test,y_pred3))\n",
    "#start_time = time.time()\n",
    "scores = cross_val_score(clf3, X_train, y_train, cv=10, scoring='precision')\n",
    "scores2 = cross_val_score(clf3, X_train, y_train, cv=10, scoring='recall')\n",
    "scores3 = cross_val_score(clf3, X_train, y_train, cv=10, scoring='f1')\n",
    "#print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "print(\"Precision: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))\n",
    "print(\"Recall: %0.2f (+/- %0.2f)\" % (scores2.mean(), scores2.std() * 2))\n",
    "print(\"F1-score: %0.2f (+/- %0.2f)\" % (scores3.mean(), scores3.std() * 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TFIDF+Senti - logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1, penalty='l1', solver='liblinear')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Import model\n",
    "from sklearn.linear_model import Lasso, LogisticRegression\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "X_train = text_tf_senti\n",
    "y_train = dft['Target']\n",
    "\n",
    "#Create a Classifier\n",
    "clf_TFIDF_vader = LogisticRegression(C=1, penalty='l1', solver='liblinear') # Linear Kernel\n",
    "clf_TFIDF_vader.fit(X_train, y_train)\n",
    "#Train the model using the whole data\n",
    "#scores = cross_val_score(clf_TFIDF_vader, X_train, y_train, cv=10, scoring='precision')\n",
    "#scores2 = cross_val_score(clf_TFIDF_vader, X_train, y_train, cv=10, scoring='recall')\n",
    "#scores3 = cross_val_score(clf_TFIDF_vader, X_train, y_train, cv=10, scoring='f1')\n",
    "#print(\"Precision: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))\n",
    "#print(\"Recall: %0.2f (+/- %0.2f)\" % (scores2.mean(), scores2.std() * 2))\n",
    "#print(\"F1-score: %0.2f (+/- %0.2f)\" % (scores3.mean(), scores3.std() * 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TFIDF-SMOTE-logistic regression (CV = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.82 (+/- 0.06)\n",
      "Recall: 0.83 (+/- 0.01)\n",
      "F1-score: 0.82 (+/- 0.03)\n"
     ]
    }
   ],
   "source": [
    "#Import model\n",
    "from sklearn.linear_model import Lasso, LogisticRegression\n",
    "from sklearn import metrics\n",
    "from numpy import mean\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from imblearn.pipeline import Pipeline\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "model = LogisticRegression(C=1, penalty='l1', solver='liblinear') # Linear Kernel\n",
    "oversample = SMOTE()\n",
    "X, y = oversample.fit_resample(X_train, y_train)\n",
    "\n",
    "scores = cross_val_score(model, X, y, cv=10, scoring='precision')\n",
    "scores2 = cross_val_score(model, X, y, cv=10, scoring='recall')\n",
    "scores3 = cross_val_score(model, X, y, cv=10, scoring='f1')\n",
    "print(\"Precision: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))\n",
    "print(\"Recall: %0.2f (+/- %0.2f)\" % (scores2.mean(), scores2.std() * 2))\n",
    "print(\"F1-score: %0.2f (+/- %0.2f)\" % (scores3.mean(), scores3.std() * 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TFIDF-SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 11.964690923690796 seconds ---\n",
      "F1-score: 0.78 (+/- 0.09)\n"
     ]
    }
   ],
   "source": [
    "clf4 = SVC(kernel = 'linear')\n",
    "start_time = time.time()\n",
    "scores = cross_val_score(clf4, X_train, y_train, cv=10, scoring='f1')\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "#scores\n",
    "print(\"F1-score: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TFIDF-Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 3.51294207572937 seconds ---\n",
      "F1-score: 0.64 (+/- 0.11)\n"
     ]
    }
   ],
   "source": [
    "clf5 = DecisionTreeClassifier()\n",
    "start_time = time.time()\n",
    "scores = cross_val_score(clf5, X_train, y_train, cv=10, scoring='f1')\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "#scores\n",
    "print(\"F1-score: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TFIDF-VotingClassifier (only use 10,000 samples; memmory isn't enough for all samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 178.06399607658386 seconds ---\n",
      "F1-score: 0.76 (+/- 0.09)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "clf1 = LogisticRegression(C=1, penalty='l1', solver='liblinear')\n",
    "clf2 = MultinomialNB() # other\n",
    "clf3 = RandomForestClassifier(n_estimators=50, random_state=1)\n",
    "clf4 = SVC(kernel = 'linear')\n",
    "clf5 = DecisionTreeClassifier()\n",
    "eclf = VotingClassifier(estimators=[('lr', clf1),('svc',clf4),('rf', clf3), ('gnb', clf2),('dt',clf5)],voting='hard')\n",
    "\n",
    "#start_time = time.time()\n",
    "scores = cross_val_score(eclf, X_train.toarray(), y_train, cv=10, scoring='precision')\n",
    "scores2 = cross_val_score(eclf, X_train.toarray(), y_train, cv=10, scoring='recall')\n",
    "scores3 = cross_val_score(eclf, X_train.toarray(), y_train, cv=10, scoring='f1')\n",
    "#print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "#scores\n",
    "print(\"Precision: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))\n",
    "print(\"Recall: %0.2f (+/- %0.2f)\" % (scores2.mean(), scores2.std() * 2))\n",
    "print(\"F1-score: %0.2f (+/- %0.2f)\" % (scores3.mean(), scores3.std() * 2))\n",
    "#eclf.fit(X_train.toarray(), y_train)\n",
    "#y_pred_V = eclf.predict(X_test.toarray())\n",
    "#print('Late Fusion- LR,GNB,RF')\n",
    "#print(classification_report(y_test,y_pred_V))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TFIDF-Late Fusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alpha1 =  0.0 alpha2 =  0.0 alpha3 =  0.0\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.63      0.62      0.63      5774\n",
      "           1       0.68      0.69      0.69      6830\n",
      "\n",
      "    accuracy                           0.66     12604\n",
      "   macro avg       0.66      0.66      0.66     12604\n",
      "weighted avg       0.66      0.66      0.66     12604\n",
      "\n",
      "alpha1 =  0.0 alpha2 =  0.0 alpha3 =  0.25\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.63      0.62      0.63      5774\n",
      "           1       0.68      0.69      0.69      6830\n",
      "\n",
      "    accuracy                           0.66     12604\n",
      "   macro avg       0.66      0.66      0.66     12604\n",
      "weighted avg       0.66      0.66      0.66     12604\n",
      "\n",
      "alpha1 =  0.0 alpha2 =  0.0 alpha3 =  0.5\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.63      0.62      0.63      5774\n",
      "           1       0.68      0.69      0.69      6830\n",
      "\n",
      "    accuracy                           0.66     12604\n",
      "   macro avg       0.66      0.66      0.66     12604\n",
      "weighted avg       0.66      0.66      0.66     12604\n",
      "\n",
      "alpha1 =  0.0 alpha2 =  0.0 alpha3 =  0.75\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.64      0.66      5774\n",
      "           1       0.71      0.76      0.74      6830\n",
      "\n",
      "    accuracy                           0.70     12604\n",
      "   macro avg       0.70      0.70      0.70     12604\n",
      "weighted avg       0.70      0.70      0.70     12604\n",
      "\n",
      "alpha1 =  0.0 alpha2 =  0.0 alpha3 =  1.0\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.60      0.67      5774\n",
      "           1       0.72      0.84      0.77      6830\n",
      "\n",
      "    accuracy                           0.73     12604\n",
      "   macro avg       0.74      0.72      0.72     12604\n",
      "weighted avg       0.74      0.73      0.73     12604\n",
      "\n",
      "alpha1 =  0.0 alpha2 =  0.25 alpha3 =  0.0\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.63      0.62      0.63      5774\n",
      "           1       0.68      0.69      0.69      6830\n",
      "\n",
      "    accuracy                           0.66     12604\n",
      "   macro avg       0.66      0.66      0.66     12604\n",
      "weighted avg       0.66      0.66      0.66     12604\n",
      "\n",
      "alpha1 =  0.0 alpha2 =  0.25 alpha3 =  0.25\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.63      0.62      0.63      5774\n",
      "           1       0.68      0.69      0.69      6830\n",
      "\n",
      "    accuracy                           0.66     12604\n",
      "   macro avg       0.66      0.66      0.66     12604\n",
      "weighted avg       0.66      0.66      0.66     12604\n",
      "\n",
      "alpha1 =  0.0 alpha2 =  0.25 alpha3 =  0.5\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.64      0.67      5774\n",
      "           1       0.72      0.78      0.75      6830\n",
      "\n",
      "    accuracy                           0.71     12604\n",
      "   macro avg       0.71      0.71      0.71     12604\n",
      "weighted avg       0.71      0.71      0.71     12604\n",
      "\n",
      "alpha1 =  0.0 alpha2 =  0.25 alpha3 =  0.75\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.62      0.69      5774\n",
      "           1       0.72      0.84      0.78      6830\n",
      "\n",
      "    accuracy                           0.74     12604\n",
      "   macro avg       0.75      0.73      0.73     12604\n",
      "weighted avg       0.74      0.74      0.74     12604\n",
      "\n",
      "alpha1 =  0.0 alpha2 =  0.5 alpha3 =  0.0\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.63      0.62      0.63      5774\n",
      "           1       0.68      0.69      0.69      6830\n",
      "\n",
      "    accuracy                           0.66     12604\n",
      "   macro avg       0.66      0.66      0.66     12604\n",
      "weighted avg       0.66      0.66      0.66     12604\n",
      "\n",
      "alpha1 =  0.0 alpha2 =  0.5 alpha3 =  0.25\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.63      0.68      5774\n",
      "           1       0.72      0.80      0.76      6830\n",
      "\n",
      "    accuracy                           0.72     12604\n",
      "   macro avg       0.72      0.72      0.72     12604\n",
      "weighted avg       0.72      0.72      0.72     12604\n",
      "\n",
      "alpha1 =  0.0 alpha2 =  0.5 alpha3 =  0.5\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.61      0.68      5774\n",
      "           1       0.72      0.84      0.78      6830\n",
      "\n",
      "    accuracy                           0.74     12604\n",
      "   macro avg       0.74      0.73      0.73     12604\n",
      "weighted avg       0.74      0.74      0.73     12604\n",
      "\n",
      "alpha1 =  0.0 alpha2 =  0.75 alpha3 =  0.0\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      0.62      0.67      5774\n",
      "           1       0.72      0.81      0.76      6830\n",
      "\n",
      "    accuracy                           0.72     12604\n",
      "   macro avg       0.73      0.72      0.72     12604\n",
      "weighted avg       0.72      0.72      0.72     12604\n",
      "\n",
      "alpha1 =  0.0 alpha2 =  0.75 alpha3 =  0.25\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.60      0.67      5774\n",
      "           1       0.71      0.83      0.77      6830\n",
      "\n",
      "    accuracy                           0.73     12604\n",
      "   macro avg       0.73      0.72      0.72     12604\n",
      "weighted avg       0.73      0.73      0.72     12604\n",
      "\n",
      "alpha1 =  0.0 alpha2 =  1.0 alpha3 =  0.0\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      0.59      0.65      5774\n",
      "           1       0.70      0.83      0.76      6830\n",
      "\n",
      "    accuracy                           0.72     12604\n",
      "   macro avg       0.72      0.71      0.71     12604\n",
      "weighted avg       0.72      0.72      0.71     12604\n",
      "\n",
      "alpha1 =  0.25 alpha2 =  0.0 alpha3 =  0.0\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.63      0.62      0.63      5774\n",
      "           1       0.68      0.69      0.69      6830\n",
      "\n",
      "    accuracy                           0.66     12604\n",
      "   macro avg       0.66      0.66      0.66     12604\n",
      "weighted avg       0.66      0.66      0.66     12604\n",
      "\n",
      "alpha1 =  0.25 alpha2 =  0.0 alpha3 =  0.25\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.63      0.62      0.63      5774\n",
      "           1       0.68      0.69      0.69      6830\n",
      "\n",
      "    accuracy                           0.66     12604\n",
      "   macro avg       0.66      0.66      0.66     12604\n",
      "weighted avg       0.66      0.66      0.66     12604\n",
      "\n",
      "alpha1 =  0.25 alpha2 =  0.0 alpha3 =  0.5\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.68      0.71      5774\n",
      "           1       0.75      0.79      0.77      6830\n",
      "\n",
      "    accuracy                           0.74     12604\n",
      "   macro avg       0.74      0.74      0.74     12604\n",
      "weighted avg       0.74      0.74      0.74     12604\n",
      "\n",
      "alpha1 =  0.25 alpha2 =  0.0 alpha3 =  0.75\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.72      0.76      5774\n",
      "           1       0.78      0.85      0.81      6830\n",
      "\n",
      "    accuracy                           0.79     12604\n",
      "   macro avg       0.79      0.78      0.79     12604\n",
      "weighted avg       0.79      0.79      0.79     12604\n",
      "\n",
      "alpha1 =  0.25 alpha2 =  0.25 alpha3 =  0.0\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.63      0.62      0.63      5774\n",
      "           1       0.68      0.69      0.69      6830\n",
      "\n",
      "    accuracy                           0.66     12604\n",
      "   macro avg       0.66      0.66      0.66     12604\n",
      "weighted avg       0.66      0.66      0.66     12604\n",
      "\n",
      "alpha1 =  0.25 alpha2 =  0.25 alpha3 =  0.25\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.68      0.72      5774\n",
      "           1       0.75      0.81      0.78      6830\n",
      "\n",
      "    accuracy                           0.75     12604\n",
      "   macro avg       0.75      0.75      0.75     12604\n",
      "weighted avg       0.75      0.75      0.75     12604\n",
      "\n",
      "alpha1 =  0.25 alpha2 =  0.25 alpha3 =  0.5\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.71      0.75      5774\n",
      "           1       0.78      0.86      0.81      6830\n",
      "\n",
      "    accuracy                           0.79     12604\n",
      "   macro avg       0.79      0.78      0.78     12604\n",
      "weighted avg       0.79      0.79      0.79     12604\n",
      "\n",
      "alpha1 =  0.25 alpha2 =  0.5 alpha3 =  0.0\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.68      0.72      5774\n",
      "           1       0.75      0.82      0.79      6830\n",
      "\n",
      "    accuracy                           0.76     12604\n",
      "   macro avg       0.76      0.75      0.75     12604\n",
      "weighted avg       0.76      0.76      0.76     12604\n",
      "\n",
      "alpha1 =  0.25 alpha2 =  0.5 alpha3 =  0.25\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.69      0.74      5774\n",
      "           1       0.76      0.86      0.81      6830\n",
      "\n",
      "    accuracy                           0.78     12604\n",
      "   macro avg       0.78      0.77      0.77     12604\n",
      "weighted avg       0.78      0.78      0.78     12604\n",
      "\n",
      "alpha1 =  0.25 alpha2 =  0.75 alpha3 =  0.0\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.67      0.72      5774\n",
      "           1       0.75      0.85      0.80      6830\n",
      "\n",
      "    accuracy                           0.77     12604\n",
      "   macro avg       0.77      0.76      0.76     12604\n",
      "weighted avg       0.77      0.77      0.76     12604\n",
      "\n",
      "alpha1 =  0.5 alpha2 =  0.0 alpha3 =  0.0\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.63      0.62      0.63      5774\n",
      "           1       0.68      0.69      0.69      6830\n",
      "\n",
      "    accuracy                           0.66     12604\n",
      "   macro avg       0.66      0.66      0.66     12604\n",
      "weighted avg       0.66      0.66      0.66     12604\n",
      "\n",
      "alpha1 =  0.5 alpha2 =  0.0 alpha3 =  0.25\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.73      0.75      5774\n",
      "           1       0.78      0.81      0.80      6830\n",
      "\n",
      "    accuracy                           0.77     12604\n",
      "   macro avg       0.77      0.77      0.77     12604\n",
      "weighted avg       0.77      0.77      0.77     12604\n",
      "\n",
      "alpha1 =  0.5 alpha2 =  0.0 alpha3 =  0.5\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.76      0.78      5774\n",
      "           1       0.80      0.85      0.83      6830\n",
      "\n",
      "    accuracy                           0.81     12604\n",
      "   macro avg       0.81      0.80      0.80     12604\n",
      "weighted avg       0.81      0.81      0.81     12604\n",
      "\n",
      "alpha1 =  0.5 alpha2 =  0.25 alpha3 =  0.0\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.72      0.75      5774\n",
      "           1       0.78      0.82      0.80      6830\n",
      "\n",
      "    accuracy                           0.78     12604\n",
      "   macro avg       0.78      0.77      0.78     12604\n",
      "weighted avg       0.78      0.78      0.78     12604\n",
      "\n",
      "alpha1 =  0.5 alpha2 =  0.25 alpha3 =  0.25\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.75      0.78      5774\n",
      "           1       0.80      0.86      0.83      6830\n",
      "\n",
      "    accuracy                           0.81     12604\n",
      "   macro avg       0.81      0.80      0.80     12604\n",
      "weighted avg       0.81      0.81      0.81     12604\n",
      "\n",
      "alpha1 =  0.5 alpha2 =  0.5 alpha3 =  0.0\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.73      0.77      5774\n",
      "           1       0.79      0.86      0.82      6830\n",
      "\n",
      "    accuracy                           0.80     12604\n",
      "   macro avg       0.80      0.79      0.79     12604\n",
      "weighted avg       0.80      0.80      0.80     12604\n",
      "\n",
      "alpha1 =  0.75 alpha2 =  0.0 alpha3 =  0.0\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.75      0.77      5774\n",
      "           1       0.80      0.82      0.81      6830\n",
      "\n",
      "    accuracy                           0.79     12604\n",
      "   macro avg       0.79      0.79      0.79     12604\n",
      "weighted avg       0.79      0.79      0.79     12604\n",
      "\n",
      "alpha1 =  0.75 alpha2 =  0.0 alpha3 =  0.25\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.77      0.79      5774\n",
      "           1       0.81      0.84      0.83      6830\n",
      "\n",
      "    accuracy                           0.81     12604\n",
      "   macro avg       0.81      0.81      0.81     12604\n",
      "weighted avg       0.81      0.81      0.81     12604\n",
      "\n",
      "alpha1 =  0.75 alpha2 =  0.25 alpha3 =  0.0\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.76      0.79      5774\n",
      "           1       0.81      0.85      0.83      6830\n",
      "\n",
      "    accuracy                           0.81     12604\n",
      "   macro avg       0.81      0.81      0.81     12604\n",
      "weighted avg       0.81      0.81      0.81     12604\n",
      "\n",
      "alpha1 =  1.0 alpha2 =  0.0 alpha3 =  0.0\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.78      0.79      5774\n",
      "           1       0.82      0.84      0.83      6830\n",
      "\n",
      "    accuracy                           0.81     12604\n",
      "   macro avg       0.81      0.81      0.81     12604\n",
      "weighted avg       0.81      0.81      0.81     12604\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# late fusion\n",
    "\n",
    "clflg = LogisticRegression(C=1, penalty='l1', solver='liblinear')\n",
    "clfnb = MultinomialNB()\n",
    "clfrf = RandomForestClassifier(n_estimators=50, random_state=1)\n",
    "clfdt = DecisionTreeClassifier(random_state=0)\n",
    "\n",
    "# fit training data\n",
    "clflg.fit(X_train, y_train)\n",
    "clfnb.fit(X_train, y_train)\n",
    "clfrf.fit(X_train, y_train)\n",
    "clfdt.fit(X_train, y_train)\n",
    "\n",
    "alphas = np.linspace(0, 1, 5)\n",
    "#print(alphas)\n",
    "clfs = [clflg, clfnb, clfrf, clfdt]\n",
    "\n",
    "# probability threshold\n",
    "pt = 0.5\n",
    "\n",
    "for alpha1 in alphas:\n",
    "    for alpha2 in alphas:\n",
    "        if(alpha1 + alpha2>1):\n",
    "            continue\n",
    "        for alpha3 in alphas:\n",
    "            if(alpha1 + alpha2 + alpha3 > 1):\n",
    "                continue\n",
    "            prob = (alpha1*clfs[0].predict_proba(X_test) + \\\n",
    "                    alpha2*clfs[1].predict_proba(X_test) + \\\n",
    "                    alpha3*clfs[2].predict_proba(X_test) + \\\n",
    "                    (1 - alpha1 - alpha2 - alpha3)*clfs[3].predict_proba(X_test))[:, 1]\n",
    "            #print(prob)\n",
    "            y_pred = np.zeros(len(prob))\n",
    "            idx = (prob>=pt)\n",
    "            y_pred[idx] = 1\n",
    "            print('alpha1 = ', alpha1, 'alpha2 = ', alpha2, 'alpha3 = ', alpha3)\n",
    "            print(classification_report(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 1, ..., 0, 0, 0])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_V"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TFIDF-LogisticRegression GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        12\n",
      "           1       0.40      1.00      0.57         8\n",
      "\n",
      "    accuracy                           0.40        20\n",
      "   macro avg       0.20      0.50      0.29        20\n",
      "weighted avg       0.16      0.40      0.23        20\n",
      "\n",
      "{'mean_fit_time': array([0.0029882 , 0.01823843]), 'std_fit_time': array([0.00099729, 0.01105432]), 'mean_score_time': array([0.00181797, 0.00306613]), 'std_score_time': array([0.00099287, 0.00285274]), 'param_C': masked_array(data=[1.0, 100.0],\n",
      "             mask=[False, False],\n",
      "       fill_value='?',\n",
      "            dtype=object), 'params': [{'C': 1.0}, {'C': 100.0}], 'split0_test_score': array([0.66666667, 0.5       ]), 'split1_test_score': array([0.66666667, 0.85714286]), 'split2_test_score': array([0.66666667, 0.57142857]), 'split3_test_score': array([0.66666667, 0.8       ]), 'split4_test_score': array([0.66666667, 0.5       ]), 'split5_test_score': array([0.66666667, 0.57142857]), 'split6_test_score': array([0.76923077, 0.54545455]), 'split7_test_score': array([0.76923077, 0.66666667]), 'split8_test_score': array([0.76923077, 0.8       ]), 'split9_test_score': array([0.76923077, 0.6       ]), 'mean_test_score': array([0.70769231, 0.64121212]), 'std_test_score': array([0.05024594, 0.12577375]), 'rank_test_score': array([1, 2], dtype=int32)}\n",
      "Best parameters from gridsearch: {'C': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chenjiqing/anaconda3/envs/Class/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Lasso, LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "#from imblearn.over_sampling import SMOTE\n",
    "#oversample = SMOTE()\n",
    "#X, y = oversample.fit_resample(X_train, y_train)\n",
    "clf3 = LogisticRegression(penalty='l1', solver='liblinear')\n",
    "params = {'C': [1.0, 100.0]}\n",
    "grid = GridSearchCV(estimator=clf3, param_grid=params, cv=10, scoring = 'f1')\n",
    "grid.fit(X_train, y_train)\n",
    "y_pred_V = grid.predict(X_test)\n",
    "print(classification_report(y_test,y_pred_V))\n",
    "print(grid.cv_results_)\n",
    "print(\"Best parameters from gridsearch: {}\".format(grid.best_params_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction csv (not includes SMOTE / Late Fusion / GridSearch / cross-validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "toy_test = pd.read_json('/Users/chenjiqing/Public/2020_Fall_term/data/Toys_and_Games_Reviews_test.json', lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>verified</th>\n",
       "      <th>reviewTime</th>\n",
       "      <th>reviewerID</th>\n",
       "      <th>asin</th>\n",
       "      <th>reviewerName</th>\n",
       "      <th>reviewText</th>\n",
       "      <th>summary</th>\n",
       "      <th>unixReviewTime</th>\n",
       "      <th>vote</th>\n",
       "      <th>style</th>\n",
       "      <th>image</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>True</td>\n",
       "      <td>01 9, 2014</td>\n",
       "      <td>F65C6B80659643241B55D2F8606CA762</td>\n",
       "      <td>B53842B7FB95A7EED604789C4527FED4</td>\n",
       "      <td>C13E13DA2073260C2194C15D782E86A9</td>\n",
       "      <td>I bought this item for my granddaughter and sh...</td>\n",
       "      <td>xmas present</td>\n",
       "      <td>1389225600</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>True</td>\n",
       "      <td>01 7, 2014</td>\n",
       "      <td>B59B4F985BFB64754C6DA13A1825344C</td>\n",
       "      <td>B53842B7FB95A7EED604789C4527FED4</td>\n",
       "      <td>EB6C1EA748A0926CF57E2915043A1B2C</td>\n",
       "      <td>My kids didn't care for this. It's not exactly...</td>\n",
       "      <td>Not what I expected, my kids didn't care for it.</td>\n",
       "      <td>1389052800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>True</td>\n",
       "      <td>06 17, 2014</td>\n",
       "      <td>7CA53EA426A3925851477B594C14C1B1</td>\n",
       "      <td>B53842B7FB95A7EED604789C4527FED4</td>\n",
       "      <td>3C05450791D51827DF4ADAC95DB284C3</td>\n",
       "      <td>Don't waste time or money on this, it does not...</td>\n",
       "      <td>Does not work</td>\n",
       "      <td>1402963200</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>True</td>\n",
       "      <td>03 26, 2014</td>\n",
       "      <td>2421E7DA831836AC70B3599A07CC6813</td>\n",
       "      <td>B53842B7FB95A7EED604789C4527FED4</td>\n",
       "      <td>EFB1465497243F7027F6D6C101F3CDDC</td>\n",
       "      <td>This is a fun idea but unfortunately the handl...</td>\n",
       "      <td>Fun idea</td>\n",
       "      <td>1395792000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>False</td>\n",
       "      <td>02 22, 2014</td>\n",
       "      <td>3A27A6CCDFEB41598C949DEE4B130773</td>\n",
       "      <td>B53842B7FB95A7EED604789C4527FED4</td>\n",
       "      <td>3CF804E7182AB12879C33C914E1C5CD8</td>\n",
       "      <td>This is a very sturdy tub toy and has handled ...</td>\n",
       "      <td>Super toy and very durable.</td>\n",
       "      <td>1393027200</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   verified   reviewTime                        reviewerID  \\\n",
       "0      True   01 9, 2014  F65C6B80659643241B55D2F8606CA762   \n",
       "1      True   01 7, 2014  B59B4F985BFB64754C6DA13A1825344C   \n",
       "2      True  06 17, 2014  7CA53EA426A3925851477B594C14C1B1   \n",
       "3      True  03 26, 2014  2421E7DA831836AC70B3599A07CC6813   \n",
       "4     False  02 22, 2014  3A27A6CCDFEB41598C949DEE4B130773   \n",
       "\n",
       "                               asin                      reviewerName  \\\n",
       "0  B53842B7FB95A7EED604789C4527FED4  C13E13DA2073260C2194C15D782E86A9   \n",
       "1  B53842B7FB95A7EED604789C4527FED4  EB6C1EA748A0926CF57E2915043A1B2C   \n",
       "2  B53842B7FB95A7EED604789C4527FED4  3C05450791D51827DF4ADAC95DB284C3   \n",
       "3  B53842B7FB95A7EED604789C4527FED4  EFB1465497243F7027F6D6C101F3CDDC   \n",
       "4  B53842B7FB95A7EED604789C4527FED4  3CF804E7182AB12879C33C914E1C5CD8   \n",
       "\n",
       "                                          reviewText  \\\n",
       "0  I bought this item for my granddaughter and sh...   \n",
       "1  My kids didn't care for this. It's not exactly...   \n",
       "2  Don't waste time or money on this, it does not...   \n",
       "3  This is a fun idea but unfortunately the handl...   \n",
       "4  This is a very sturdy tub toy and has handled ...   \n",
       "\n",
       "                                            summary  unixReviewTime vote  \\\n",
       "0                                      xmas present      1389225600  NaN   \n",
       "1  Not what I expected, my kids didn't care for it.      1389052800  NaN   \n",
       "2                                     Does not work      1402963200  NaN   \n",
       "3                                          Fun idea      1395792000  NaN   \n",
       "4                       Super toy and very durable.      1393027200  NaN   \n",
       "\n",
       "  style image  \n",
       "0   NaN   NaN  \n",
       "1   NaN   NaN  \n",
       "2   NaN   NaN  \n",
       "3   NaN   NaN  \n",
       "4   NaN   NaN  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toy_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chenjiqing/anaconda3/envs/Class/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3146: DtypeWarning: Columns (10) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment_scores</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>asin</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>00027761289A98A2884B5F152D3DBF91</th>\n",
       "      <td>0.382740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>000D550D33ABEA8BAF3C8E0FD88C1707</th>\n",
       "      <td>0.579656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0024011FC5EBE2E578E9032EC2EEE3EA</th>\n",
       "      <td>0.527580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>00255AA8E14A573176D24AC68524541E</th>\n",
       "      <td>0.678940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>00290349FBB97F31976D0471975A4789</th>\n",
       "      <td>0.570060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FFE0E00A3198C981F461CA920E1BE2E0</th>\n",
       "      <td>0.863050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FFE23C8B29D211456D9774F49E277905</th>\n",
       "      <td>0.439300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FFF41906931B0E2371E123299500D8E5</th>\n",
       "      <td>0.364782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FFF574CC84FD43D875FF7257B3797BF3</th>\n",
       "      <td>0.595000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FFF970B0EBBEA7752C6BAEBE3F4361D8</th>\n",
       "      <td>0.582300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>15755 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  sentiment_scores\n",
       "asin                                              \n",
       "00027761289A98A2884B5F152D3DBF91          0.382740\n",
       "000D550D33ABEA8BAF3C8E0FD88C1707          0.579656\n",
       "0024011FC5EBE2E578E9032EC2EEE3EA          0.527580\n",
       "00255AA8E14A573176D24AC68524541E          0.678940\n",
       "00290349FBB97F31976D0471975A4789          0.570060\n",
       "...                                            ...\n",
       "FFE0E00A3198C981F461CA920E1BE2E0          0.863050\n",
       "FFE23C8B29D211456D9774F49E277905          0.439300\n",
       "FFF41906931B0E2371E123299500D8E5          0.364782\n",
       "FFF574CC84FD43D875FF7257B3797BF3          0.595000\n",
       "FFF970B0EBBEA7752C6BAEBE3F4361D8          0.582300\n",
       "\n",
       "[15755 rows x 1 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vader_sntimentscore_test = pd.read_csv('dft_test_vaderSentimentAdded.csv')\n",
    "avg_vader_sntimentscore_test = vader_sntimentscore_test[['asin', 'sentiment_scores']].groupby('asin').mean()\n",
    "avg_vader_sntimentscore_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(367913, 2)\n",
      "(15755, 2)\n",
      "                               asin  \\\n",
      "0  00027761289A98A2884B5F152D3DBF91   \n",
      "1  000D550D33ABEA8BAF3C8E0FD88C1707   \n",
      "2  0024011FC5EBE2E578E9032EC2EEE3EA   \n",
      "3  00255AA8E14A573176D24AC68524541E   \n",
      "4  00290349FBB97F31976D0471975A4789   \n",
      "\n",
      "                                          reviewText  \n",
      "0  Bought for a contest. Although for a child it ...  \n",
      "1  I do a lot of model kit building, mainly warsh...  \n",
      "2  I bought this game for the kids at my son's 2n...  \n",
      "3  I bought this as a gift and the recipient abso...  \n",
      "4  Daughter likes this very much. Cheap alternati...  \n"
     ]
    }
   ],
   "source": [
    "# select coloumns\n",
    "toy_test2 = toy_test[['asin', 'reviewText']]\n",
    "clean_toy_test = toy_test2[toy_test2['reviewText'].notna()]\n",
    "print(clean_toy_test.shape)\n",
    "clean_toy_test2 = clean_toy_test.groupby('asin')['reviewText'].apply(' '.join).reset_index()\n",
    "clean_toy_test2.head()\n",
    "print(clean_toy_test2.shape)\n",
    "print(clean_toy_test2.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 151962)\t0.023741331568156925\n",
      "  (0, 150233)\t0.04345880328538978\n",
      "  (0, 149852)\t0.09747095777753212\n",
      "  (0, 149579)\t0.06048949819576235\n",
      "  (0, 149535)\t0.06183491648218076\n",
      "  (0, 149032)\t0.2778927754827474\n",
      "  (0, 149007)\t0.4872246723760914\n",
      "  (0, 148765)\t0.051447773837739984\n",
      "  (0, 148460)\t0.027844353062162683\n",
      "  (0, 148136)\t0.06312839290438638\n",
      "  (0, 148060)\t0.025540837543542732\n",
      "  (0, 147699)\t0.02744615601266229\n",
      "  (0, 147326)\t0.04353980785843322\n",
      "  (0, 144443)\t0.03814688138154437\n",
      "  (0, 144436)\t0.06730124544369397\n",
      "  (0, 143947)\t0.05292031195780934\n",
      "  (0, 138762)\t0.044955461570473816\n",
      "  (0, 138179)\t0.05761758813169907\n",
      "  (0, 136830)\t0.01981248823203037\n",
      "  (0, 136132)\t0.2620215065391931\n",
      "  (0, 134375)\t0.06675042782912839\n",
      "  (0, 133683)\t0.15166240771362147\n",
      "  (0, 130133)\t0.07600260619822456\n",
      "  (0, 128897)\t0.09791091504848393\n",
      "  (0, 126167)\t0.03350106024236067\n",
      "  :\t:\n",
      "  (15730, 152986)\t0.7702571428571429\n",
      "  (15731, 152986)\t0.4531125\n",
      "  (15732, 152986)\t0.5680615384615384\n",
      "  (15733, 152986)\t0.6554560137457043\n",
      "  (15734, 152986)\t0.42231578947368426\n",
      "  (15735, 152986)\t0.5184333333333334\n",
      "  (15736, 152986)\t0.3114100000000001\n",
      "  (15737, 152986)\t0.35990000000000005\n",
      "  (15738, 152986)\t0.7886545454545455\n",
      "  (15739, 152986)\t0.571209090909091\n",
      "  (15740, 152986)\t0.7495625\n",
      "  (15741, 152986)\t0.7823166666666667\n",
      "  (15742, 152986)\t0.7590500000000001\n",
      "  (15743, 152986)\t0.6523555555555557\n",
      "  (15744, 152986)\t0.7324428571428572\n",
      "  (15745, 152986)\t0.5402285714285714\n",
      "  (15746, 152986)\t0.8311285714285714\n",
      "  (15747, 152986)\t0.9182333333333333\n",
      "  (15748, 152986)\t0.6364363636363636\n",
      "  (15749, 152986)\t0.6282\n",
      "  (15750, 152986)\t0.8630500000000001\n",
      "  (15751, 152986)\t0.4393000000000001\n",
      "  (15752, 152986)\t0.3647823529411764\n",
      "  (15753, 152986)\t0.595\n",
      "  (15754, 152986)\t0.5822999999999999\n"
     ]
    }
   ],
   "source": [
    "tf2=TfidfVectorizer(vocabulary = tf.vocabulary_)\n",
    "text_tf2= tf2.fit_transform(clean_toy_test2['reviewText'])\n",
    "text_tf_senti_test = sparse.hstack((text_tf2,np.array(avg_vader_sntimentscore_test['sentiment_scores'])[:,None]))\n",
    "print(text_tf_senti_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_toy_test_pred = clf_TFIDF_vader.predict(text_tf_senti_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 0 ... 0 1 0]\n"
     ]
    }
   ],
   "source": [
    "print(y_toy_test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                   asin\n",
      "0      00027761289A98A2884B5F152D3DBF91\n",
      "1      000D550D33ABEA8BAF3C8E0FD88C1707\n",
      "2      0024011FC5EBE2E578E9032EC2EEE3EA\n",
      "3      00255AA8E14A573176D24AC68524541E\n",
      "4      00290349FBB97F31976D0471975A4789\n",
      "...                                 ...\n",
      "15750  FFE0E00A3198C981F461CA920E1BE2E0\n",
      "15751  FFE23C8B29D211456D9774F49E277905\n",
      "15752  FFF41906931B0E2371E123299500D8E5\n",
      "15753  FFF574CC84FD43D875FF7257B3797BF3\n",
      "15754  FFF970B0EBBEA7752C6BAEBE3F4361D8\n",
      "\n",
      "[15755 rows x 1 columns]\n",
      "                                   asin  preds\n",
      "0      00027761289A98A2884B5F152D3DBF91      0\n",
      "1      000D550D33ABEA8BAF3C8E0FD88C1707      1\n",
      "2      0024011FC5EBE2E578E9032EC2EEE3EA      0\n",
      "3      00255AA8E14A573176D24AC68524541E      1\n",
      "4      00290349FBB97F31976D0471975A4789      1\n",
      "...                                 ...    ...\n",
      "15750  FFE0E00A3198C981F461CA920E1BE2E0      0\n",
      "15751  FFE23C8B29D211456D9774F49E277905      1\n",
      "15752  FFF41906931B0E2371E123299500D8E5      0\n",
      "15753  FFF574CC84FD43D875FF7257B3797BF3      1\n",
      "15754  FFF970B0EBBEA7752C6BAEBE3F4361D8      0\n",
      "\n",
      "[15755 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "dft_predict1 = clean_toy_test2[['asin']]\n",
    "print(dft_predict1)\n",
    "dft_predict1['preds'] = y_toy_test_pred\n",
    "print(dft_predict1)\n",
    "dft_predict1.to_csv('predictions.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
