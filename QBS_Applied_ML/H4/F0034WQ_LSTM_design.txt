- First, I calculated the text length for each review. Next, I removed punctuation, numbers, and special characters, and lowercase the text. Then, en_core_web_sm in spacy package was used for tokenization. After tokenization, I used Counter() to count the number of occurences of each word, and then removed infrequent words (word count < 2).

- Before model training, I created a vocabulary to index mapping and encode our review text. The maximum length of any review is 20 words because the average length of reviews was around 10 (so I decided the maximum length be twice the average length of reviews).

- I split train_yelp_reviews into training and validation set with ratio 8:2. The first layer is the embedding layer, and its input dimension is vocabulary size = 826, and output dimsension of embedding dimension is 100. The second layer is the LSTM layer, its input is the output from the embedding layer, and the dimension of the output of LSTM is 100. For LSTM, I used bidirectional training, and the rate of dropout was 0.3. The final layer is the linear layer, and its input dimension is 100; output dimension is 2. Loss function would be cross-entropy loss; optimizer would be Adam optimizer. The epochs was 125, and learning rate was 0.01